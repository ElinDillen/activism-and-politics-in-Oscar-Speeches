{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xasdWePSwjkr",
        "JNBx3UFFwtiP",
        "8N9xXqa7wlME"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOC9VI7MF19L0E8Ut0EAfKh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElinDillen/activism-and-politics-in-Oscar-Speeches/blob/main/Models_to_test_out.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Roberta and sentence-transformers"
      ],
      "metadata": {
        "id": "xasdWePSwjkr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6Im06IKtNRA"
      },
      "outputs": [],
      "source": [
        "#this code embeds the text automatically and uses roberta-base or other models to analyze text+our linguistic features\n",
        "#simplistic code to test things out\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Assuming you have text and features\n",
        "# df = pd.read_csv(\"your_data.csv\")\n",
        "\n",
        "# Choose a pre-trained model\n",
        "model_name = \"roberta-base\" # Or roberta-base or sentence-transformers/all-MiniLM-L6-v2\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Create a dataset that combines text and features\n",
        "class CombinedFeaturesDataset(Dataset):\n",
        "    def __init__(self, texts, linguistic_features, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.linguistic_features = linguistic_features\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Tokenize text\n",
        "        encoding = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Convert to appropriate format and remove batch dimension\n",
        "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
        "\n",
        "        # Add linguistic features\n",
        "        item[\"linguistic_features\"] = torch.tensor(self.linguistic_features[idx], dtype=torch.float)\n",
        "\n",
        "        # Add label\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "\n",
        "        return item\n",
        "\n",
        "# Split your data\n",
        "texts = df['text'].values\n",
        "features = df[['verb_count', 'hedge_count', 'article_count', 'sentiment_score']].values\n",
        "labels = df['personality_label'].values\n",
        "\n",
        "X_train_texts, X_test_texts, X_train_features, X_test_features, y_train, y_test = train_test_split(\n",
        "    texts, features, labels, test_size=0.2\n",
        ")\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = CombinedFeaturesDataset(X_train_texts, X_train_features, y_train, tokenizer)\n",
        "test_dataset = CombinedFeaturesDataset(X_test_texts, X_test_features, y_test, tokenizer)\n",
        "\n",
        "# Custom model that combines text embeddings with linguistic features\n",
        "class CombinedPersonalityModel(torch.nn.Module):\n",
        "    def __init__(self, model_name, linguistic_feature_size, num_labels):\n",
        "        super().__init__()\n",
        "        self.text_encoder = AutoModel.from_pretrained(model_name)\n",
        "        self.text_encoder_dim = self.text_encoder.config.hidden_size\n",
        "\n",
        "        # Fusion layer\n",
        "        self.fusion = torch.nn.Sequential(\n",
        "            torch.nn.Linear(self.text_encoder_dim + linguistic_feature_size, 256),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(0.1),\n",
        "            torch.nn.Linear(256, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, linguistic_features, labels=None):\n",
        "        # Get text embeddings\n",
        "        text_outputs = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        text_embedding = text_outputs.last_hidden_state[:, 0, :]  # CLS token\n",
        "\n",
        "        # Concatenate with linguistic features\n",
        "        combined_features = torch.cat([text_embedding, linguistic_features], dim=1)\n",
        "\n",
        "        # Classification\n",
        "        logits = self.fusion(combined_features)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = torch.nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits, labels)\n",
        "\n",
        "        return torch.nn.functional.softmax(logits, dim=1) if loss is None else (loss, logits)\n",
        "\n",
        "# Initialize the model\n",
        "model = CombinedPersonalityModel(\n",
        "    model_name=model_name,\n",
        "    linguistic_feature_size=X_train_features.shape[1],\n",
        "    num_labels=len(np.unique(labels))\n",
        ")\n",
        "\n",
        "# Define a custom Trainer to handle your combined inputs\n",
        "class CombinedTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            linguistic_features=inputs[\"linguistic_features\"],\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        loss = outputs[0] if isinstance(outputs, tuple) else outputs\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./combined_personality_model\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "# Set up trainer\n",
        "trainer = CombinedTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "model_save_path = \"./combined_personality_model_final\"\n",
        "trainer.save_model(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "print(f\"Model and tokenizer saved to {model_save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to load and use the model\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load the saved model and tokenizer\n",
        "model_path = \"./combined_personality_model_final\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = CombinedPersonalityModel.from_pretrained(model_path)\n",
        "\n",
        "# Set to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Function to make predictions with the loaded model\n",
        "def predict_personality(text, linguistic_features):\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        max_length=128,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Convert linguistic features to tensor\n",
        "    features_tensor = torch.tensor([linguistic_features], dtype=torch.float)\n",
        "\n",
        "    # Get prediction\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            linguistic_features=features_tensor,\n",
        "            labels=None\n",
        "        )\n",
        "\n",
        "    # Get predicted class\n",
        "    prediction = torch.argmax(outputs, dim=1).item()\n",
        "    return prediction"
      ],
      "metadata": {
        "id": "rIMmU3bltWRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Minej/bert-base-personality\n",
        "\n",
        "https://huggingface.co/Minej/bert-base-personality"
      ],
      "metadata": {
        "id": "JNBx3UFFwtiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import BertTokenizer, BertModel, Trainer, TrainingArguments\n",
        "\n",
        "# Load tokenizer and encoder\n",
        "model_name = \"Minej/bert-base-personality\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "bert_encoder = BertModel.from_pretrained(model_name)\n",
        "\n",
        "# --- Per-word normalization of linguistic features ---\n",
        "df[\"word_count\"] = df[\"text\"].apply(lambda x: len(str(x).split()))\n",
        "# List of raw features\n",
        "raw_feats = [\"hedge_count\", \"verb_count\", \"article_count\", \"sentiment_score\"]\n",
        "# Create per-word versions\n",
        "for col in raw_feats:\n",
        "    df[col + \"_per_word\"] = df[col] / df[\"word_count\"].clip(lower=1)\n",
        "\n",
        "# Final features to use\n",
        "feature_cols = [f + \"_per_word\" for f in raw_feats]\n",
        "linguistic_features = df[feature_cols].values\n",
        "\n",
        "# Optional: scale features for stability\n",
        "scaler = StandardScaler()\n",
        "linguistic_features = scaler.fit_transform(linguistic_features)\n",
        "\n",
        "# Labels: multi-label binarized columns\n",
        "label_cols = ['Extroversion', 'Neuroticism', 'Agreeableness', 'Conscientiousness', 'Openness']\n",
        "labels = df[label_cols].values\n",
        "\n",
        "# Text\n",
        "texts = df[\"text\"].values\n",
        "\n",
        "# --- Dataset class ---\n",
        "class CombinedPersonalityDataset(Dataset):\n",
        "    def __init__(self, texts, linguistic_features, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.features = linguistic_features\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
        "        item[\"linguistic_features\"] = torch.tensor(self.features[idx], dtype=torch.float)\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "        return item\n",
        "\n",
        "# --- Model class ---\n",
        "class CombinedPersonalityModel(torch.nn.Module):\n",
        "    def __init__(self, bert_model, linguistic_feature_size, num_traits=5):\n",
        "        super().__init__()\n",
        "        self.text_encoder = bert_model\n",
        "        self.text_dim = self.text_encoder.config.hidden_size\n",
        "\n",
        "        self.fusion = torch.nn.Sequential(\n",
        "            torch.nn.Linear(self.text_dim + linguistic_feature_size, 256),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(0.2),\n",
        "            torch.nn.Linear(256, num_traits)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, linguistic_features, labels=None):\n",
        "        with torch.no_grad():  # Freeze BERT if desired\n",
        "            outputs = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            cls_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS]\n",
        "\n",
        "        combined = torch.cat([cls_embedding, linguistic_features], dim=1)\n",
        "        logits = self.fusion(combined)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "            loss = loss_fn(logits, labels)\n",
        "\n",
        "        return (loss, logits) if loss is not None else torch.sigmoid(logits)\n",
        "\n",
        "# --- Split data ---\n",
        "X_train, X_test, F_train, F_test, Y_train, Y_test = train_test_split(\n",
        "    texts, linguistic_features, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "train_dataset = CombinedPersonalityDataset(X_train, F_train, Y_train, tokenizer)\n",
        "test_dataset = CombinedPersonalityDataset(X_test, F_test, Y_test, tokenizer)\n",
        "\n",
        "# --- Trainer setup ---\n",
        "class CombinedTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        loss, logits = model(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            linguistic_features=inputs[\"linguistic_features\"],\n",
        "            labels=labels\n",
        "        )\n",
        "        return (loss, (logits, labels)) if return_outputs else loss\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    from sklearn.metrics import f1_score, accuracy_score\n",
        "    logits, labels = eval_pred\n",
        "    preds = (torch.sigmoid(torch.tensor(logits)) > 0.5).int().numpy()\n",
        "    return {\n",
        "        \"f1\": f1_score(labels, preds, average=\"macro\"),\n",
        "        \"accuracy\": accuracy_score(labels, preds)\n",
        "    }\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./minej_personality_model\",\n",
        "    num_train_epochs=4,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        ")\n",
        "\n",
        "# --- Train ---\n",
        "model = CombinedPersonalityModel(\n",
        "    bert_model=bert_encoder,\n",
        "    linguistic_feature_size=F_train.shape[1],\n",
        "    num_traits=labels.shape[1]\n",
        ")\n",
        "\n",
        "trainer = CombinedTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# --- Save ---\n",
        "trainer.save_model(\"./minej_personality_combined_model\")\n",
        "tokenizer.save_pretrained(\"./minej_personality_combined_model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "HXeG5cJsxznu",
        "outputId": "038c4f7b-6480-49af-cc70-a23b1c84282f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5e1b3d5bcf2b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m from ._classification_threshold import (\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mFixedThresholdClassifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mTunedThresholdClassifierCV\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_classification_threshold.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m from ..metrics import (\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mget_scorer_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m from ._classification import (\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mbalanced_accuracy_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Flan-T5"
      ],
      "metadata": {
        "id": "8N9xXqa7wlME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saw a comment that this works well and wanted to try it out. Does not work without fine-tuning."
      ],
      "metadata": {
        "id": "iw3JCDUo1FJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkJoCrb9yARO",
        "outputId": "c870d826-5d8d-4c73-d54e-57524e46835f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "def prompt_personality(text):\n",
        "    prompt = f\"\"\"Given this text: \\\"{text}\\\", classify the user's personality based on the Big Five traits (Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism). Provide a list of the top 5 most prominent traits with a brief explanation for each.\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "PmXsa3wRwu70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_personality(\"I have been presented with many situations that put me outside of my comfort zone while living abroad. One of it's key aspects is the inevitable confrontation with language conflict. Even if you speak a bit of language, encountering and interacting with the locals will remain challenging. These situations have made me more adaptable, more receptive to what words and explanations are more easily understood. One of the ways to overcome this barrier was getting to know the key cultural concepts or elements that people from a specific country/region/city know, may it be a specific word, a song, a movie, a saying, random fact about the local football team - all these things seem to create a connection, as it shows that you care and are curious about the people surrounding you. I have recently started volunteering at a cultural center in which my tasks include bartending. I have no previous experience of bartending and the majority of visitors speak a language that I do not know. Even when I applied for this position I knew that I will inevitably makes mistakes, just as one always does when starting a new thing. During my first shifts, I miscalculated prices, poured drinks to the wrong cups, had to deal with foamy and fussy beers and had to learn how to deal with less satisfied visitors.This experience, together with many other volunteering opportunities that I had done in the past, mainly taught to not be afraid of making mistakes, as it is the only way to learn. Additionally, I have noticed is that in such instances, I always have to slow myself down, calm my mind and pay attention to what I am doing, this way not letting the stress in and helping me learn and perform better. Finally and perhaps most importantly, the place and the people determine the actual experience and it's perception. If you are in a good place, that is welcoming and understanding, where you feel appreciated, you are less likely to make mistakes and even if you do, the lesson will be learned much more quickly. Perhaps the most challenging leadership role I had to take on was becoming a curator for my previous university's student representations group. My task included finding students who would represent their courses and would try to improve the study experience in the meetings with professors. The main issue I encountered the initial fear of reaching out to people and the perception of my role itself. The student group I was in, was often acting as a very bureaucratic entity that would not make any impactful change and I felt that some of the students had the same view of me.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "sk8fZCqxx9Va",
        "outputId": "6fb4ae16-cb88-4f19-dee7-645501b27122"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Openness Conscientiousness Extraversion Agreeableness Neuroticism'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}